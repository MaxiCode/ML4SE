{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each instance of this class stores a single fittness rule\n",
    "class Fitness_Rule :  \n",
    "    # takes a string which containes the rule and builds the rule\n",
    "    def __init__(self, line) :\n",
    "        self.option_list = []\n",
    "        line_list = line.split('#')\n",
    "        for element in line_list :\n",
    "            if element.count(':') == 0 :\n",
    "                self.option_list.append(element)\n",
    "            else :\n",
    "                self.option_list.append(element[:element.find(':')])\n",
    "                self.fitnes_value = float(element[element.find(':')+2:])    \n",
    "                \n",
    "    # print overload\n",
    "    def __str__(self) :\n",
    "        out = \"\"\n",
    "        for element in self.option_list :\n",
    "            out += element\n",
    "            out += \" ** \"\n",
    "        out = out[:-3]\n",
    "        out += \" : \" + str(self.fitnes_value)\n",
    "        return out\n",
    "        \n",
    "    # takes an option dictionariy and returns the cost of this rule\n",
    "    def get_partial_fitness(self, option_activation) :\n",
    "        applicable = 1\n",
    "        for x in self.option_list :\n",
    "            applicable = applicable and option_activation[x]\n",
    "        return applicable * self.fitnes_value        \n",
    "\n",
    "    \n",
    "# contains all fittness rules\n",
    "class Fitness_Modell :\n",
    "    # takes a string containing fittness rules and adds them to the rule list\n",
    "    def add_fitness_rules(self, fittness_input) :\n",
    "        for line in fittness_input :\n",
    "            self.fittness_rule_list.append(Fitness_Rule(line))\n",
    "            \n",
    "\n",
    "    # takes a list containing fittness rules and builds the rule list\n",
    "    def __init__(self) :\n",
    "        self.fittness_rule_list = []\n",
    "        \n",
    "                        \n",
    "    # print overload\n",
    "    def __str__(self) :\n",
    "        out = \"\"\n",
    "        for element in self.fittness_rule_list :\n",
    "            out += element.__str__() + \"\\n\"\n",
    "        out = out[:-2]\n",
    "        return out\n",
    "    \n",
    "    # takes an option activation dictionary and calculates the fitness(or cost) value\n",
    "    def calculate_fitness(self, option_activation) : \n",
    "        fittness = 0.0\n",
    "        for element in self.fittness_rule_list :\n",
    "            fittness += element.get_partial_fitness(option_activation)\n",
    "        return fittness\n",
    "\n",
    "\n",
    "\n",
    "# takes a dictionary and re\n",
    "#def calculate fitness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an instance of this class stores all constrains\n",
    "# TODO: optimize validity check\n",
    "class Constraint_Model :\n",
    "        \n",
    "    def __init__(self) :\n",
    "        self.constraint_list = []\n",
    "        self.global_tabu_list = []\n",
    "        self.mandatory_activation = []\n",
    "                 \n",
    "    # input line(string) list\n",
    "    def build_model(self, dimacs_input) :\n",
    "        self.constraint_list = []\n",
    "        self.global_tabu_list = []\n",
    "        self.mandatory_activation = []\n",
    "        tmp_constraint = []\n",
    "        \n",
    "        dimacs_input = [x.strip() for x in dimacs_input]\n",
    "        for line in dimacs_input :\n",
    "            # fill constraint list \n",
    "            if len(line) > 0 and line[0] != 'c' and line[0] != 'p' :\n",
    "                line_list = line.split(' ')\n",
    "                for element in line_list :\n",
    "                    if element == '0' and len(element) > 0 :\n",
    "                        self.constraint_list.append(tmp_constraint)\n",
    "                        tmp_constraint = []\n",
    "                    elif len(element) > 0 :\n",
    "                        tmp_constraint.append(int(element))      \n",
    "            # initialize tabu list and mandatory activation\n",
    "            elif len(line) > 0 and line[0] == 'p' :\n",
    "                line_list = line.split(' ')\n",
    "                self.global_tabu_list = [False] * (int(line_list[2]) + 1)\n",
    "                self.mandatory_activation = [None] * (int(line_list[2]) + 1)\n",
    "                \n",
    "                \n",
    "    # print function            \n",
    "    def __str__ (self) :\n",
    "        out = \"Constraints:\\n\"\n",
    "        for a in self.constraint_list :\n",
    "            out += str(a) + \"-\"+ str(len(a)) + \", \"\n",
    "        \n",
    "        out += \"\\n\\nGLobal Tabu List:\\n\"\n",
    "        out += str(self.global_tabu_list)\n",
    "        \n",
    "        out += \"\\n\\nMandatory Activation List:\\n\"\n",
    "        out += str(self.mandatory_activation)\n",
    "           \n",
    "        return out\n",
    "        \n",
    "    \n",
    "    # builds the global tabu list and sets the activation values \n",
    "    def build_global_tabu_list(self) :\n",
    "        new_constraint_list = []\n",
    "        for constraint in self.constraint_list :  \n",
    "            if len(constraint) == 1 :       \n",
    "                self.global_tabu_list[abs(constraint[0])] = True      \n",
    "                self.mandatory_activation[abs(constraint[0])] = constraint[0] > 0    \n",
    "            else :\n",
    "                new_constraint_list.append(constraint)\n",
    "                \n",
    "        self.constraint_list = new_constraint_list\n",
    "#        for constraint in self.constraint_list :\n",
    "#            if len(constraint) == 2 :\n",
    "    \n",
    "    \n",
    "    # TODO: optimize !!!!!!\n",
    "    def get_violated_variables (self, activation_arry) :\n",
    "        violated_variables = []\n",
    "        for constraint in self.constraint_list :\n",
    "            if not self.check_partial_validity(constraint, activation_arry) :\n",
    "                for v in constraint :\n",
    "                    if abs(v) not in violated_variables :\n",
    "                        violated_variables.append(abs(v))\n",
    "        violated_variables.sort()\n",
    "        return violated_variables\n",
    "    \n",
    "    \n",
    "    # TODO: optimize !!!!!!    \n",
    "    def check_validity (self, activation_arry) :\n",
    "        for constraint in self.constraint_list :\n",
    "            if not self.check_partial_validity(constraint, activation_arry) :\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    # TODO: optimize\n",
    "    def check_partial_validity (self, constraint, activation_arry) :\n",
    "        for x in constraint :\n",
    "            if x > 0 :\n",
    "                if activation_arry[x] :\n",
    "                    return True\n",
    "            else :\n",
    "                 if not activation_arry[x * (-1)] :\n",
    "                    return True   \n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(n) :\n",
    "    import itertools\n",
    "    combinations = list(map(list, itertools.product([0, 1], repeat=n)))\n",
    "    return(combinations)\n",
    "\n",
    "def parse_xml_to_dimacs(file):\n",
    "    var_map = [] #\n",
    "\n",
    "    final_string = \"\" #\n",
    "    variables_string = \"\" #\n",
    "    p_line = \"p cnf \" #\n",
    "    clauses = \"\" #\n",
    "    current_options = [] # \n",
    "    optional_false_elements = []\n",
    "    clauses_2D = []\n",
    "\n",
    "    from lxml import etree\n",
    "    xml_rules = etree.parse(open(file, 'r'))\n",
    "    root = xml_rules.getroot()\n",
    "\n",
    "    # first iteration to generate the dimacs variables\n",
    "    for child in root :\n",
    "        for subchild in child :\n",
    "            for element in subchild : \n",
    "                if element.tag == 'name':\n",
    "                    var_map.append(element.text)\n",
    "\n",
    "    # generate variables\n",
    "    for el in var_map:\n",
    "        variables_string += \"c \" + str(1+var_map.index(el)) + \" \" + el + \"\\n\"\n",
    "\n",
    "    # generate rules\n",
    "    for child in root :\n",
    "        for subchild in child :\n",
    "            for element in subchild :\n",
    "\n",
    "                # temporary save current element and excluded options\n",
    "                if element.tag == 'name':\n",
    "                    current_options.append(element.text)\n",
    "                if element.tag == 'excludedOptions' :\n",
    "                    for option in element :\n",
    "                        current_options.append(option.text)\n",
    "\n",
    "                # clear temp if 'otional' tag is set to true\n",
    "                if element.tag == 'optional' :\n",
    "                    if element.text != 'False' :\n",
    "                        current_options = []\n",
    "\n",
    "            # if there are choices to do generate all combinations at once\n",
    "            if len(current_options) > 0 :\n",
    "                ###########################################################################\n",
    "                # adding all combinations of rules if they are not already in \n",
    "                ###########################################################################\n",
    "                # current_name, current_options\n",
    "                #\n",
    "                # print(\"Inside combinations loop\", str(len(current_options)))\n",
    "\n",
    "                if len(current_options) == 1 :\n",
    "                    #print(\"exact one element\")\n",
    "                    for element in current_options:\n",
    "                        #print(\"1\")\n",
    "                        tmp=[]\n",
    "                        tmp.append(1+var_map.index(element))\n",
    "                        clauses_2D.append(tmp)\n",
    "\n",
    "                if len(current_options) > 1 :\n",
    "                    ####################################################current_options = current_options.sort()\n",
    "                    combinations = linear_regression(len(current_options))\n",
    "                    for lst in combinations :\n",
    "                        if not lst.count(False) == 1 :\n",
    "                            tmp_list = []\n",
    "                            for el, el2 in zip(lst, current_options) :\n",
    "                                if el :\n",
    "                                    tmp_list.append(1+var_map.index(el2))\n",
    "                                else :\n",
    "                                    tmp_list.append((1+var_map.index(el2))*(-1))\n",
    "\n",
    "                            # do not put duplicate elements in list\n",
    "                            tmp_list.sort()\n",
    "                            is_in_list = False\n",
    "                            for item in clauses_2D:\n",
    "                                if item == tmp_list:\n",
    "                                    is_in_list = True\n",
    "                            if not is_in_list :\n",
    "                                clauses_2D.append(tmp_list)\n",
    "\n",
    "            current_options = []\n",
    "\n",
    "    p_line += str(len(var_map))+\" \"+str(len(clauses_2D))+\"\\n\"\n",
    "\n",
    "    #print(clauses_2D)\n",
    "    for line in clauses_2D :\n",
    "        for item in line :\n",
    "            clauses += str(item)+\" \"\n",
    "        clauses += \"0\\n\"\n",
    "\n",
    "    # und zack fertig .... dimacs\n",
    "    final_string = variables_string+p_line+clauses\n",
    "    return final_string.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Wrapper :\n",
    "    \n",
    "    def __init__(self, path_to_folder, project_name) :\n",
    "        from os import listdir\n",
    "        from os.path import isfile, join\n",
    "        \n",
    "        \n",
    "        self.fitness_model = Fitness_Modell()\n",
    "        self.constraint_model = Constraint_Model()\n",
    "        \n",
    "        self.best = []\n",
    "        self.popsize = 10\n",
    "        self.population = []\n",
    "        self.fittness_list = []\n",
    "       \n",
    "        files = [join(path_to_folder, f) for f in listdir(path_to_folder) if isfile(join(path_to_folder, f))]\n",
    "        \n",
    "        for file in files:\n",
    "             # -------- Fitness Model -----------\n",
    "            if file.count(project_name) > 0 and (file.count('feature') > 0 or file.count('interactions') > 0) :\n",
    "                with open(file, 'r') as f:\n",
    "                    self.fitness_model.add_fitness_rules(f.readlines())\n",
    "            \n",
    "             # ------ Constraint Model ---------\n",
    "            if file.count(project_name) > 0 and file.count('.xml') > 0 :\n",
    "                self.constraint_model.build_model(parse_xml_to_dimacs(file))\n",
    "            if file.count(project_name) > 0 and file.count('.dimacs') > 0 :\n",
    "                with open(file, 'r') as f:\n",
    "                    self.constraint_model.build_model(f.readlines())\n",
    "                    \n",
    "            # analyze constraints\n",
    "            self.constraint_model.build_global_tabu_list()\n",
    "\n",
    "            \n",
    "    def init_population(self) :\n",
    "        random.seed()\n",
    "        for i in range(self.popsize) :\n",
    "            self.population.append(self.init_individual())\n",
    "            \n",
    "        tmp_list = []\n",
    "        for it in self.population :\n",
    "            if not it == None :\n",
    "                tmp_list.append(it)\n",
    "                \n",
    "        self.population = tmp_list\n",
    "        self.popsize = len(self.population)\n",
    "        \n",
    "        for i in range(0, self.popsize) :\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    def init_individual(self) :     \n",
    "        configuration = copy.deepcopy(self.constraint_model.mandatory_activation)\n",
    "        tabu_config = copy.deepcopy(self.constraint_model.global_tabu_list)\n",
    "        constraint_list = copy.deepcopy(self.constraint_model.constraint_list)\n",
    "        \n",
    "        is_valid = False\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        while not is_valid :\n",
    "            for idx in range(len(configuration)) :\n",
    "                if not tabu_config[idx]:\n",
    "                    configuration[idx] = bool(random.getrandbits(1))\n",
    "                    \n",
    "            tmp = self.constraint_model.get_violated_variables(configuration)\n",
    "            ptr = 0\n",
    "            #print(\"length tmp: \" + str(len(tmp)))\n",
    "            for it in tmp :\n",
    "                for i in range(ptr + 1, it - 1) :\n",
    "                    tabu_config[i] = True\n",
    "                tabu_config[it] = False\n",
    "                ptr = it\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if self.constraint_model.check_validity(configuration)  :\n",
    "                is_valid = True\n",
    "                print (counter)\n",
    "                return configuration\n",
    "            if counter > 10000 : \n",
    "                print (counter)\n",
    "                return None\n",
    "            \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-7a15f85536f0>, line 86)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-7a15f85536f0>\"\u001b[0;36m, line \u001b[0;32m86\u001b[0m\n\u001b[0;31m    def evolution (self, wrapperinstanz)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Helper_Functions :\n",
    "    # This class contains needed functions to construct an machine learning algorithm\n",
    "    \n",
    "    \n",
    "   # def __init__(self) :\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tweak(self, individual, constraint_model) :\n",
    "        # tries to tweak an individual by randomly modify its elements \n",
    "        # (we want to stay in the valid area, so check constraints)\n",
    "        \n",
    "        tweaked_individual = copy.deepcopy(individual)\n",
    "        \n",
    "        is_valid = False\n",
    "        counter = 0\n",
    "        element_to_change = 0\n",
    "        ind_len = len(individual)\n",
    "        \n",
    "        while not is_valid:\n",
    "            element_to_change = random.randint(1, ind_len)\n",
    "            \n",
    "            if tweaked_individual[element_to_change] :\n",
    "                tweaked_individual[element_to_change] = False\n",
    "            else :\n",
    "                tweaked_individual[element_to_change] = True\n",
    "            \n",
    "            if constraint_model.check_validity(tweaked_individual):\n",
    "                return tweaked_individual\n",
    "            else :\n",
    "                # if not valid change back \n",
    "                if tweaked_individual[element_to_change] :\n",
    "                    tweaked_individual[element_to_change] = False\n",
    "                else :\n",
    "                    tweaked_individual[element_to_change] = True\n",
    "            \n",
    "            counter += 1\n",
    "            if counter > 1000 :                \n",
    "                print(\"Failed tweak\")\n",
    "                return individual\n",
    "        \n",
    "        return individual\n",
    "    \n",
    "    \n",
    "    def quality(self, individual, fitness_model) :\n",
    "        # calgulates quality of an individual\n",
    "        \n",
    "        return fittness_model.calculate_fitness(individual)\n",
    "        \n",
    "    \n",
    "    def crossover(self, parent1, parent2) :\n",
    "        # generates two children by crossovering two parents\n",
    "        \n",
    "        if len(parent1) is not len(parent2) :\n",
    "            print(\"size of both individuals should be equal to do crossover\")\n",
    "            return\n",
    "        \n",
    "        child1 = []\n",
    "        child2 = []\n",
    "        \n",
    "        for el1, el2 in zip(parent1, parent2) :\n",
    "            if bool(random.getrandbits(1)) :\n",
    "                child1.append(el2)\n",
    "                child2.append(el1)\n",
    "            else :\n",
    "                child1.append(el1)\n",
    "                child2.append(el2)\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    \n",
    "    def diversity(self, individual1, individual2) :\n",
    "        # compares equality of two individuals element by element and return weighted sum\n",
    "        \n",
    "        if len(individual1) is not len(individual2) :\n",
    "            print(\"size of both individuals should be equal to compare\")\n",
    "            return\n",
    "        \n",
    "        elements_equal = 0\n",
    "        for el1, el2 in zip(individual1, individual2) :\n",
    "            if el1 == el2 :\n",
    "                elements_equal += 1\n",
    "\n",
    "        return elements_equal/len(individual1)\n",
    "    \n",
    "    def sel\n",
    "    \n",
    "    def evolution (self, wrapperinstanz, number_of_iterations) :\n",
    "        random.seed()\n",
    "        for i in range(0, number_of_iterations) :\n",
    "            print(\"blub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926\n",
      "188\n",
      "731\n",
      "94\n",
      "278\n",
      "655\n",
      "351\n",
      "625\n",
      "1141\n",
      "566\n",
      "10\n",
      "Constraints:\n",
      "[-137, -49]-2, [-137, -50]-2, [-138, -49]-2, [-138, -50]-2, [-140, -45]-2, [-140, -46]-2, [-141, -45]-2, [-141, -46]-2, [-143, -29]-2, [-143, -30]-2, [-144, -29]-2, [-144, -30]-2, [-146, 7]-2, [-146, 8]-2, [-147, -7]-2, [-147, -8]-2, [-149, 35]-2, [-149, 36]-2, [-150, -35]-2, [-150, -36]-2, [-152, 103]-2, [-152, 104]-2, [-153, -103]-2, [-153, -104]-2, [-155, 9]-2, [-155, 10]-2, [-156, -9]-2, [-156, -10]-2, [-158, 115]-2, [-158, 116]-2, [-159, -115]-2, [-159, -116]-2, [-161, 117]-2, [-161, 118]-2, [-162, -117]-2, [-162, -118]-2, [-164, 125]-2, [-164, 126]-2, [-165, -125]-2, [-165, -126]-2, [-167, 67]-2, [-167, 68]-2, [-168, -67]-2, [-168, -68]-2, [-170, 47]-2, [-170, 48]-2, [-171, -47]-2, [-171, -48]-2, [-173, 25]-2, [-173, 26]-2, [-174, -25]-2, [-174, -26]-2, [-69, 25, 26]-3, [-70, 25, 26]-3, [-69, -70, -25, 26]-4, [-176, -25]-2, [-176, -26]-2, [25, 26, 176]-3, [-176, -25, -69, 25]-4, [-176, -25, -25, 69]-4, [-176, -25, -70, 26]-4, [-176, -25, -26, 70]-4, [-177, 176]-2, [-177, -25]-2, [-177, -26]-2, [-176, 25, 26, 177]-4, [-177, -69]-2, [-177, -70]-2, [-179, 41]-2, [-179, 42]-2, [-180, -41]-2, [-180, -42]-2, [-182, 1]-2, [-182, 2]-2, [-183, -1]-2, [-183, -2]-2, [-185, 105]-2, [-185, 106]-2, [-186, -105]-2, [-186, -106]-2, [-188, 59]-2, [-188, 60]-2, [-189, -59]-2, [-189, -60]-2, [-191, -123]-2, [-191, -124]-2, [-192, -123]-2, [-192, -124]-2, [-194, 79]-2, [-194, 80]-2, [-195, -79]-2, [-195, -80]-2, [-131, 79, 80]-3, [-132, 79, 80]-3, [-131, -132, -79, 80]-4, [-197, -79]-2, [-197, -80]-2, [79, 80, 197]-3, [-197, -79, -131, 79]-4, [-197, -79, -79, 131]-4, [-197, -79, -132, 80]-4, [-197, -79, -80, 132]-4, [-198, 197]-2, [-198, -79]-2, [-198, -80]-2, [-197, 79, 80, 198]-4, [-198, -131]-2, [-198, -132]-2, [-200, 23]-2, [-200, 24]-2, [-201, -23]-2, [-201, -24]-2, [-55, 23, 24]-3, [-56, 23, 24]-3, [-55, -56, -23, 24]-4, [-203, -23]-2, [-203, -24]-2, [23, 24, 203]-3, [-203, -23, -55, 23]-4, [-203, -23, -23, 55]-4, [-203, -23, -56, 24]-4, [-203, -23, -24, 56]-4, [-204, 203]-2, [-204, -23]-2, [-204, -24]-2, [-203, 23, 24, 204]-4, [-204, -55]-2, [-204, -56]-2, [-206, -57]-2, [-206, -58]-2, [-207, -57]-2, [-207, -58]-2, [-65, 57, 58]-3, [-66, 57, 58]-3, [-65, -66, -57, 58]-4, [-209, -57]-2, [-209, -58]-2, [57, 58, 209]-3, [-209, -57, -65]-3, [-209, -57, -66]-3, [-210, 209]-2, [-210, -57]-2, [-210, -58]-2, [-209, 57, 58, 210]-4, [-210, -65]-2, [-210, -66]-2, [-85, 57, 58]-3, [-86, 57, 58]-3, [-85, -86, -57, 58]-4, [-212, -57]-2, [-212, -58]-2, [57, 58, 212]-3, [-212, -57, -85]-3, [-212, -57, -86]-3, [-213, 212]-2, [-213, -57]-2, [-213, -58]-2, [-212, 57, 58, 213]-4, [-213, -85]-2, [-213, -86]-2, [-15, 57, 58]-3, [-16, 57, 58]-3, [-15, -16, -57, 58]-4, [-215, -57]-2, [-215, -58]-2, [57, 58, 215]-3, [-215, -57, -15]-3, [-215, -57, -16]-3, [-216, 215]-2, [-216, -57]-2, [-216, -58]-2, [-215, 57, 58, 216]-4, [-216, -15]-2, [-216, -16]-2, [-43, 57, 58]-3, [-44, 57, 58]-3, [-43, -44, -57, 58]-4, [-218, -57]-2, [-218, -58]-2, [57, 58, 218]-3, [-218, -57, -43]-3, [-218, -57, -44]-3, [-219, 218]-2, [-219, -57]-2, [-219, -58]-2, [-218, 57, 58, 219]-4, [-219, -43]-2, [-219, -44]-2, [-221, 129]-2, [-221, 130]-2, [-222, -129]-2, [-222, -130]-2, [-224, 73]-2, [-224, 74]-2, [-225, -73]-2, [-225, -74]-2, [-227, 99]-2, [-227, 100]-2, [-228, -99]-2, [-228, -100]-2, [-51, 99, 100]-3, [-52, 99, 100]-3, [-51, -52, -99, 100]-4, [-230, -99]-2, [-230, -100]-2, [99, 100, 230]-3, [-230, -99, -51, 99]-4, [-230, -99, -99, 51]-4, [-230, -99, -52, 100]-4, [-230, -99, -100, 52]-4, [-231, 230]-2, [-231, -99]-2, [-231, -100]-2, [-230, 99, 100, 231]-4, [-231, -51]-2, [-231, -52]-2, [-233, 37]-2, [-233, 38]-2, [-234, -37]-2, [-234, -38]-2, [-236, 5]-2, [-236, 6]-2, [-237, -5]-2, [-237, -6]-2, [-239, 89]-2, [-239, 90]-2, [-240, -89]-2, [-240, -90]-2, [-242, -27]-2, [-242, -28]-2, [-243, -27]-2, [-243, -28]-2, [-111, 27, 28]-3, [-112, 27, 28]-3, [-111, -112, -27, 28]-4, [-245, -27]-2, [-245, -28]-2, [27, 28, 245]-3, [-245, -27, -111]-3, [-245, -27, -112]-3, [-246, 245]-2, [-246, -27]-2, [-246, -28]-2, [-245, 27, 28, 246]-4, [-246, -111]-2, [-246, -112]-2, [-248, 121]-2, [-248, 122]-2, [-249, -121]-2, [-249, -122]-2, [-251, -3]-2, [-251, -4]-2, [-252, -3]-2, [-252, -4]-2, [-254, 71]-2, [-254, 72]-2, [-255, -71]-2, [-255, -72]-2, [-257, 93]-2, [-257, 94]-2, [-258, -93]-2, [-258, -94]-2, [-260, 87]-2, [-260, 88]-2, [-261, -87]-2, [-261, -88]-2, [-263, 17]-2, [-263, 18]-2, [-264, -17]-2, [-264, -18]-2, [-127, 17, 18]-3, [-128, 17, 18]-3, [-127, -128, -17, 18]-4, [-266, -17]-2, [-266, -18]-2, [17, 18, 266]-3, [-266, -17, -127, 17]-4, [-266, -17, -17, 127]-4, [-266, -17, -128, 18]-4, [-266, -17, -18, 128]-4, [-267, 266]-2, [-267, -17]-2, [-267, -18]-2, [-266, 17, 18, 267]-4, [-267, -127]-2, [-267, -128]-2, [-269, 109]-2, [-269, 110]-2, [-270, -109]-2, [-270, -110]-2, [-272, 83]-2, [-272, 84]-2, [-273, -83]-2, [-273, -84]-2, [-275, 135]-2, [-275, 136]-2, [-276, -135]-2, [-276, -136]-2, [-278, 13]-2, [-278, 14]-2, [-279, -13]-2, [-279, -14]-2, [-31, 13, 14]-3, [-32, 13, 14]-3, [-31, -32, -13, 14]-4, [-281, -13]-2, [-281, -14]-2, [13, 14, 281]-3, [-281, -13, -31]-3, [-281, -13, -32]-3, [-282, 281]-2, [-282, -13]-2, [-282, -14]-2, [-281, 13, 14, 282]-4, [-282, -31]-2, [-282, -32]-2, [-61, 31, 32]-3, [-62, 31, 32]-3, [-61, -62, -31, 32]-4, [-284, -31]-2, [-284, -32]-2, [31, 32, 284]-3, [-284, -31, -61]-3, [-284, -31, -62]-3, [-285, 284]-2, [-285, -31]-2, [-285, -32]-2, [-284, 31, 32, 285]-4, [-285, -61]-2, [-285, -62]-2, [-39, 31, 32]-3, [-40, 31, 32]-3, [-39, -40, -31, 32]-4, [-287, -31]-2, [-287, -32]-2, [31, 32, 287]-3, [-287, -31, -39]-3, [-287, -31, -40]-3, [-288, 287]-2, [-288, -31]-2, [-288, -32]-2, [-287, 31, 32, 288]-4, [-288, -39]-2, [-288, -40]-2, [-101, 13, 14]-3, [-102, 13, 14]-3, [-101, -102, -13, 14]-4, [-290, -13]-2, [-290, -14]-2, [13, 14, 290]-3, [-290, -13, -101]-3, [-290, -13, -102]-3, [-291, 290]-2, [-291, -13]-2, [-291, -14]-2, [-290, 13, 14, 291]-4, [-291, -101]-2, [-291, -102]-2, [-119, 13, 14]-3, [-120, 13, 14]-3, [-119, -120, -13, 14]-4, [-293, -13]-2, [-293, -14]-2, [13, 14, 293]-3, [-293, -13, -119]-3, [-293, -13, -120]-3, [-294, 293]-2, [-294, -13]-2, [-294, -14]-2, [-293, 13, 14, 294]-4, [-294, -119]-2, [-294, -120]-2, [-107, 119, 120]-3, [-108, 119, 120]-3, [-107, -108, -119, 120]-4, [-296, -119]-2, [-296, -120]-2, [119, 120, 296]-3, [-296, -119, -107]-3, [-296, -119, -108]-3, [-297, 296]-2, [-297, -119]-2, [-297, -120]-2, [-296, 119, 120, 297]-4, [-297, -107]-2, [-297, -108]-2, [-21, 119, 120]-3, [-22, 119, 120]-3, [-21, -22, -119, 120]-4, [-299, -119]-2, [-299, -120]-2, [119, 120, 299]-3, [-299, -119, -21]-3, [-299, -119, -22]-3, [-300, 299]-2, [-300, -119]-2, [-300, -120]-2, [-299, 119, 120, 300]-4, [-300, -21]-2, [-300, -22]-2, [-11, 119, 120]-3, [-12, 119, 120]-3, [-11, -12, -119, 120]-4, [-302, -119]-2, [-302, -120]-2, [119, 120, 302]-3, [-302, -119, -11]-3, [-302, -119, -12]-3, [-303, 302]-2, [-303, -119]-2, [-303, -120]-2, [-302, 119, 120, 303]-4, [-303, -11]-2, [-303, -12]-2, [-81, 11, 12]-3, [-82, 11, 12]-3, [-81, -82, -11, 12]-4, [-305, -11]-2, [-305, -12]-2, [11, 12, 305]-3, [-305, -11, -81]-3, [-305, -11, -82]-3, [-306, 305]-2, [-306, -11]-2, [-306, -12]-2, [-305, 11, 12, 306]-4, [-306, -81]-2, [-306, -82]-2, [-63, 81, 82]-3, [-64, 81, 82]-3, [-63, -64, -81, 82]-4, [-308, -81]-2, [-308, -82]-2, [81, 82, 308]-3, [-308, -81, -63]-3, [-308, -81, -64]-3, [-309, 308]-2, [-309, -81]-2, [-309, -82]-2, [-308, 81, 82, 309]-4, [-309, -63]-2, [-309, -64]-2, [-75, 13, 14]-3, [-76, 13, 14]-3, [-75, -76, -13, 14]-4, [-311, -13]-2, [-311, -14]-2, [13, 14, 311]-3, [-311, -13, -75]-3, [-311, -13, -76]-3, [-312, 311]-2, [-312, -13]-2, [-312, -14]-2, [-311, 13, 14, 312]-4, [-312, -75]-2, [-312, -76]-2, [-77, 13, 14]-3, [-78, 13, 14]-3, [-77, -78, -13, 14]-4, [-314, -13]-2, [-314, -14]-2, [13, 14, 314]-3, [-314, -13, -77]-3, [-314, -13, -78]-3, [-315, 314]-2, [-315, -13]-2, [-315, -14]-2, [-314, 13, 14, 315]-4, [-315, -77]-2, [-315, -78]-2, [-33, 13, 14]-3, [-34, 13, 14]-3, [-33, -34, -13, 14]-4, [-317, -13, -33]-3, [-317, -13, -34]-3, [-318, 317]-2, [-318, -13]-2, [-318, -14]-2, [-317, 13, 14, 318]-4, [-318, -33]-2, [-318, -34]-2, [-95, 13, 14]-3, [-96, 13, 14]-3, [-95, -96, -13, 14]-4, [-320, -13, -95]-3, [-320, -13, -96]-3, [-321, 320]-2, [-321, -13]-2, [-321, -14]-2, [-320, 13, 14, 321]-4, [-321, -95]-2, [-321, -96]-2, [-97, 13, 14]-3, [-98, 13, 14]-3, [-97, -98, -13, 14]-4, [-323, -13, -97]-3, [-323, -13, -98]-3, [-324, 323]-2, [-324, -13]-2, [-324, -14]-2, [-323, 13, 14, 324]-4, [-324, -97]-2, [-324, -98]-2, [-326, 19]-2, [-326, 20]-2, [-327, -19]-2, [-327, -20]-2, [-329, 133]-2, [-329, 134]-2, [-330, -133]-2, [-330, -134]-2, [-332, 53]-2, [-332, 54]-2, [-333, -53]-2, [-333, -54]-2, [-335, 91]-2, [-335, 92]-2, [-336, -91]-2, [-336, -92]-2, [-338, 113]-2, [-338, 114]-2, [-339, -113]-2, [-339, -114]-2, [-49, 50]-2, [-45, 46]-2, [-29, 30]-2, [-7, 8]-2, [-35, 36]-2, [-103, 104]-2, [-9, 10]-2, [-115, 116]-2, [-117, 118]-2, [-125, 126]-2, [-67, 68]-2, [-47, 48]-2, [-25, 26]-2, [-69, 70]-2, [-41, 42]-2, [-1, 2]-2, [-105, 106]-2, [-59, 60]-2, [-123, 124]-2, [-79, 80]-2, [-131, 132]-2, [-23, 24]-2, [-55, 56]-2, [-57, 58]-2, [-65, 66]-2, [-85, 86]-2, [-15, 16]-2, [-43, 44]-2, [-129, 130]-2, [-73, 74]-2, [-99, 100]-2, [-51, 52]-2, [-37, 38]-2, [-5, 6]-2, [-89, 90]-2, [-27, 28]-2, [-111, 112]-2, [-121, 122]-2, [-3, 4]-2, [-71, 72]-2, [-93, 94]-2, [-87, 88]-2, [-17, 18]-2, [-127, 128]-2, [-109, 110]-2, [-83, 84]-2, [-135, 136]-2, [-13, 14]-2, [-31, 32]-2, [-61, 62]-2, [-39, 40]-2, [-101, 102]-2, [-119, 120]-2, [-107, 108]-2, [-21, 22]-2, [-11, 12]-2, [-81, 82]-2, [-63, 64]-2, [-75, 76]-2, [-77, 78]-2, [-33, 34]-2, [-95, 96]-2, [-97, 98]-2, [-19, 20]-2, [-133, 134]-2, [-53, 54]-2, [-91, 92]-2, [-113, 114]-2, [1, -2]-2, [3, -4]-2, [5, -6]-2, [7, -8]-2, [9, -10]-2, [11, -12]-2, [13, -14]-2, [15, -16]-2, [17, -18]-2, [19, -20]-2, [21, -22]-2, [23, -24]-2, [25, -26]-2, [27, -28]-2, [29, -30]-2, [31, -32]-2, [33, -34]-2, [35, -36]-2, [37, -38]-2, [39, -40]-2, [41, -42]-2, [43, -44]-2, [45, -46]-2, [47, -48]-2, [49, -50]-2, [51, -52]-2, [53, -54]-2, [55, -56]-2, [57, -58]-2, [59, -60]-2, [61, -62]-2, [63, -64]-2, [65, -66]-2, [67, -68]-2, [69, -70]-2, [71, -72]-2, [73, -74]-2, [75, -76]-2, [77, -78]-2, [79, -80]-2, [81, -82]-2, [83, -84]-2, [85, -86]-2, [87, -88]-2, [89, -90]-2, [91, -92]-2, [93, -94]-2, [95, -96]-2, [97, -98]-2, [99, -100]-2, [101, -102]-2, [103, -104]-2, [105, -106]-2, [107, -108]-2, [109, -110]-2, [111, -112]-2, [113, -114]-2, [115, -116]-2, [117, -118]-2, [119, -120]-2, [121, -122]-2, [123, -124]-2, [125, -126]-2, [127, -128]-2, [129, -130]-2, [131, -132]-2, [133, -134]-2, [135, -136]-2, \n",
      "\n",
      "GLobal Tabu List:\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, False, True, True, True, True, False, False, True, False, False, True, False, False, True, False, False, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, True, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Mandatory Activation List:\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, None, None, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, None, None, False, False, False, False, None, None, False, False, False, False, None, None, False, None, None, False, None, None, False, None, None, False, False, False, False, False, False, False, False, False, False, None, None, False, False, False, False, False, False, False, False, False, False, False, False, False, None, None, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, None, None, False, False, False, False, False, False, False, False, False, False, False, False, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, None, None, False, True, None, False, True, None, False, True, None, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "#testinstanz = Wrapper('./project_public_2/', 'busy')\n",
    "testinstanz = Wrapper('./project_public_2/', 'toy')\n",
    "#testinstanz = Wrapper('./project_public_1/', 'bdbc')\n",
    "#testinstanz = Wrapper('./project_public_1/', 'h264')\\\n",
    "\n",
    "testinstanz.init_population()\n",
    "print(len(testinstanz.population))\n",
    "learner = Helper_Functions()\n",
    "\n",
    "\n",
    "if len(testinstanz.population) > 0 :\n",
    "    print(testinstanz.constraint_model)\n",
    "    x =learner.tweak(testinstanz.population[0], testinstanz.constraint_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
